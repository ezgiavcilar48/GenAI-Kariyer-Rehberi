import streamlit as st
import os # Ortam deÄŸiÅŸkenlerini yÃ¶netmek iÃ§in
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_community.chains import ConversationalRetrievalChain


# --- 1. TEORÄ°K MÄ°MARÄ° AÃ‡IKLAMASI (PDF'in 4. AdÄ±m Gereksinimi) ---

# Bu bÃ¶lÃ¼m, PDF'de istenen "Ã‡Ã¶zÃ¼m Mimarisi"nin (4. AdÄ±m) teorik aÃ§Ä±klamasÄ±nÄ± iÃ§erir.
# Bu tarz dokÃ¼mantasyon, teknik anlatÄ±mlara notebook iÃ§erisinde veya Python dosyasÄ±nda 
# [cite_start]yorum satÄ±rlarÄ± iÃ§erisinde yer verme gereksinimini karÅŸÄ±lar[cite: 7].

"""
Ã‡Ã–ZÃœM MÄ°MARÄ°SÄ°: RAG (Retrieval Augmented Generation)

A. PROBLEM Ã‡Ã–ZÃœMÃœ:
Proje, Ã¶ÄŸrencilerin kariyer konularÄ±nda (CV, mÃ¼lakat, staj) sorduÄŸu spesifik sorulara
genel internet bilgisi yerine, **Ã¶zel olarak beslenmiÅŸ** dokÃ¼manlar Ã¼zerinden
**gÃ¼venilir** cevaplar verme problemini Ã§Ã¶zer. Bu, LLM'lerin halÃ¼sinasyon riskini azaltÄ±r.

[cite_start]B. KULLANILAN TEKNOLOJÄ°LER[cite: 23, 42, 43, 44]:
1. [cite_start]LLM (Generative Model): ChatOpenAI (Genellikle gpt-3.5-turbo kullanÄ±lÄ±r) [cite: 42]
2. [cite_start]RAG Framework: LangChain [cite: 44]
3. [cite_start]Embedding Model: Sentence Transformers (AÃ§Ä±k KaynaklÄ±) [cite: 43]
4. [cite_start]VektÃ¶r Database: Chroma (Lokal, hafif veritabanÄ±) [cite: 43]
5. ArayÃ¼z: Streamlit

C. RAG MÄ°MARÄ°SÄ° ADIMLARI:
1. Loading (YÃ¼kleme): Veri Seti (`data/` klasÃ¶rÃ¼ndeki .txt dosyalarÄ±) yÃ¼klenir.
2. Splitting (BÃ¶lme): Metinler kÃ¼Ã§Ã¼k parÃ§alara (1000 karakter) ayrÄ±lÄ±r.
3. Embedding (GÃ¶mme): Her metin parÃ§asÄ± vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
4. Storage (Depolama): Bu vektÃ¶rler, hÄ±zlÄ± arama iÃ§in ChromaDB'ye kaydedilir.
5. Retrieval (Geri Ã‡aÄŸÄ±rma): KullanÄ±cÄ± sorusuna en alakalÄ± 3 vektÃ¶r (metin parÃ§asÄ±) geri Ã§aÄŸrÄ±lÄ±r.
6. Generation (Ãœretme): Geri Ã§aÄŸrÄ±lan parÃ§alar (context) ve soru, LLM'e gÃ¶nderilerek cevap oluÅŸturulur.
"""
load_dotenv()

# --- 2. VERÄ° YÃœKLEME VE RAG Ä°LK KURULUM FONKSÄ°YONLARI ---

# RAG Sisteminin HazÄ±rlanmasÄ±
@st.cache_resource # Streamlit'in bu fonksiyonu sadece bir kere Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± saÄŸlar
def setup_rag():
    """Veri yÃ¼kleme, bÃ¶lme ve ChromaDB'ye kaydetme iÅŸlemlerini yapar."""
    
    # 1. Veri YÃ¼kleme (Loading)
    try:
        # data klasÃ¶rÃ¼ndeki tÃ¼m .txt dosyalarÄ±nÄ± yÃ¼kle
        loader = DirectoryLoader('./data/', glob="**/*.txt", loader_cls=lambda p: TextLoader(p, encoding='utf8'))
        documents = loader.load()
        
    except FileNotFoundError:
        # EÄŸer veri seti klasÃ¶rÃ¼ bulunamazsa hata gÃ¶ster
        st.error("HATA: 'data' klasÃ¶rÃ¼ bulunamadÄ± veya iÃ§inde .txt dosyasÄ± yok. LÃ¼tfen kontrol edin.")
        return None
    
    if not documents:
        st.error("HATA: Veri seti yÃ¼klenemedi. 'data' klasÃ¶rÃ¼nÃ¼zÃ¼n boÅŸ olmadÄ±ÄŸÄ±ndan emin olun.")
        return None
    
    # 2. Metin ParÃ§alama (Splitting)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    texts = text_splitter.split_documents(documents)
    
    # 3. Embedding ve VektÃ¶r DB OluÅŸturma (Embedding & Storage)
    # LangChain ile Sentence Transformer Embeddings kullanÄ±yoruz
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # Chroma DB oluÅŸturulur ve metinler vektÃ¶rleÅŸtirilerek DB'ye kaydedilir.
    # persist_directory, veritabanÄ±nÄ± diske kaydetmeye yarar.
    db = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
    db.persist() 

    # 4. LLM ve Zinciri Kurma
    # Gemini API anahtarÄ±nÄ±n tanÄ±mlÄ± olduÄŸundan emin ol
    if not os.environ.get("GEMINI_API_KEY"):
        st.error("HATA: GEMINI_API_KEY ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ. LÃ¼tfen terminalde ayarlayÄ±n.")
        return None
    
    #Gemini modelini tanÄ±mla

    llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", 
    temperature=0.0,
    google_api_key=os.environ.get("GEMINI_API_KEY"))
    retriever = db.as_retriever(search_kwargs={"k": 3}) # En alakalÄ± 3 belgeyi geri getir
    
    # KonuÅŸma tabanlÄ± RAG zincirini kur (KonuÅŸma geÃ§miÅŸini korur)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, 
        retriever=retriever, 
        return_source_documents=True,
        # KonuÅŸma geÃ§miÅŸini korumak iÃ§in bir lambda fonksiyonu
        get_chat_history=lambda h: h 
    )
    
    return qa_chain

# --- 3. STREAMLIT ARABÄ°RÄ°MÄ° ---

# Sayfa YapÄ±landÄ±rmasÄ±
st.set_page_config(page_title="ğŸ“ Kariyer AsistanÄ± Chatbot", layout="wide")
st.title("ğŸ“ Kariyer AsistanÄ±: RAG Destekli Chatbot")
st.write("CV hazÄ±rlama, mÃ¼lakat ipuÃ§larÄ± ve staj bulma konularÄ±nda size Ã¶zel rehberlik sunuyorum.")

# RAG sistemini baÅŸlatma kontrolÃ¼
if 'qa_chain' not in st.session_state:
    with st.spinner("Kariyer verileri yÃ¼kleniyor ve RAG sistemi hazÄ±rlanÄ±yor..."):
        st.session_state['qa_chain'] = setup_rag()
        st.session_state['chat_history'] = []

qa_chain = st.session_state.get('qa_chain')

# EÄŸer sistem hatasÄ±z kurulduysa devam et
if qa_chain:
    # KullanÄ±cÄ± giriÅŸi
    user_query = st.chat_input("Bir kariyer sorusu sorun (Ã–rn: STAR metodu nedir?).")

    if user_query:
        # KullanÄ±cÄ± sorusunu ve geÃ§miÅŸi zincire gÃ¶nder
        with st.spinner("Cevap aranÄ±yor..."):
             # Soruyu zincire gÃ¶nder
             result = qa_chain.invoke({"question": user_query, "chat_history": st.session_state['chat_history']})
        
        # Yeni konuÅŸmayÄ± geÃ§miÅŸe ekle
        st.session_state['chat_history'].append((user_query, result["answer"]))
        
        # Son cevabÄ±n kaynaklarÄ±nÄ± session state'e kaydet (gÃ¶stermek iÃ§in)
        st.session_state['last_result'] = result

    # Sohbet geÃ§miÅŸini gÃ¶sterme
    for question, answer in st.session_state['chat_history']:
        with st.chat_message("user"):
            st.write(question)
        with st.chat_message("assistant"):
            st.write(answer)
            
            # Geri Ã§aÄŸrÄ±lan kaynaklarÄ± gÃ¶sterme (RAG baÅŸarÄ±sÄ±nÄ±n kanÄ±tÄ±)
            if 'last_result' in st.session_state and st.session_state['chat_history'][-1][1] == answer:
                result = st.session_state['last_result']
                
                if 'source_documents' in result:
                    # Kaynak dosya adlarÄ±nÄ± al (Sadece dosya adÄ±nÄ± gÃ¶ster)
                    source_files = [doc.metadata['source'].split(os.path.sep)[-1] for doc in result['source_documents']]
                    unique_sources = list(set(source_files))
                    
                    if unique_sources:
                        st.markdown(f"**Kaynaklar:** {', '.join(unique_sources)}", help="Bu cevabÄ±n dayandÄ±ÄŸÄ± kariyer rehberliÄŸi dokÃ¼manlarÄ±.")